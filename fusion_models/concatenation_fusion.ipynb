{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041e3823",
   "metadata": {},
   "source": [
    "# Concatenation Fusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881f590",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1919e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    ViTModel\n",
    ")\n",
    "from torchvision import transforms\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58a0a7",
   "metadata": {},
   "source": [
    "## Load Paths & Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f3df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"/mnt/e/ecs289l/mimic-cxr-download/imageData/\"\n",
    "report_dir = \"/mnt/e/ecs289l/mimic-cxr-download/textData/\"\n",
    "labels_file = \"../download_data/metadata/edema+pleural_effusion_samples_v2.csv\"\n",
    "model_name_text = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "model_name_vision = 'google/vit-base-patch16-224-in21k'\n",
    "max_length = 128\n",
    "\n",
    "# ---------------------\n",
    "# Labels and File Loading\n",
    "# ---------------------\n",
    "# Load metadata\n",
    "meta = pd.read_csv(labels_file, dtype={'study_id': str})\n",
    "meta['study_id'] = 's' + meta['study_id']\n",
    "label_map = meta.set_index('study_id')[['edema', 'effusion']].to_dict(orient='index')\n",
    "\n",
    "# Collect image paths and labels\n",
    "all_image_paths = []\n",
    "for root, _, files in os.walk(image_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.dcm'):\n",
    "            all_image_paths.append(os.path.join(root, f))\n",
    "paths, labels = [], []\n",
    "for p in all_image_paths:\n",
    "    sid = os.path.basename(os.path.dirname(p))\n",
    "    if sid in label_map:\n",
    "        paths.append(p)\n",
    "        labels.append(label_map[sid]['edema'] + label_map[sid]['effusion']*2)  # placeholder, we will use list below\n",
    "# Actually build multi-label list\n",
    "labels = [ [label_map[os.path.basename(os.path.dirname(p))]['edema'],\n",
    "            label_map[os.path.basename(os.path.dirname(p))]['effusion']]\n",
    "          for p in paths ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73492b",
   "metadata": {},
   "source": [
    "## Dataset Tokenizer and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b69fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name_text)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49181c31",
   "metadata": {},
   "source": [
    "## Data Preprocess and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9493cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 7199/7199 [00:02<00:00, 3564.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "cache_dir = \"./cached_data\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def process_and_save(i, path, label):\n",
    "    try:\n",
    "        # Ignore if the file already exists\n",
    "        cache_path = os.path.join(cache_dir, f'{i}.pt')\n",
    "        if os.path.exists(cache_path):\n",
    "            return\n",
    "        # Image\n",
    "        dcm = pydicom.dcmread(path)\n",
    "        arr = dcm.pixel_array.astype(np.float32)\n",
    "        img = Image.fromarray((arr / arr.max() * 255).astype(np.uint8)).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "\n",
    "        # Text\n",
    "        sid = os.path.basename(os.path.dirname(path))\n",
    "        report_path = os.path.join(report_dir, sid, 'report.txt')\n",
    "        with open(report_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Label\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        # Save all\n",
    "        torch.save({\n",
    "            'pixel_values': img_tensor,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_tensor\n",
    "        }, os.path.join(cache_dir, f'{i}.pt'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process index {i}: {e}\")\n",
    "\n",
    "# Set number of threads based on your CPU (e.g., 8 or 16)\n",
    "num_threads = 8\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [\n",
    "        executor.submit(process_and_save, i, path, label)\n",
    "        for i, (path, label) in enumerate(zip(paths, labels))\n",
    "    ]\n",
    "\n",
    "    for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Caching\"):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2547d",
   "metadata": {},
   "source": [
    "## Fusion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7a37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, image_paths, report_dir, labels_map, tokenizer, max_length, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.report_dir = report_dir\n",
    "        self.labels_map = labels_map\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        dcm = pydicom.dcmread(p)\n",
    "        arr = dcm.pixel_array.astype(np.float32)\n",
    "        img = Image.fromarray((arr/arr.max()*255).astype(np.uint8)).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        sid = os.path.basename(os.path.dirname(p))\n",
    "        report_path = os.path.join(self.report_dir, sid, 'report.txt')\n",
    "        with open(report_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        labels = list(self.labels_map[sid].values())\n",
    "        return {\n",
    "            'pixel_values': img,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Custom collate function to batch fusion data\n",
    "def fusion_collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "class CachedFusionDataset(Dataset):\n",
    "    def __init__(self, cache_dir, indices):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.indices = indices  # list of indices used in pre-split sets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(os.path.join(self.cache_dir, f'{self.indices[idx]}.pt'))\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33864e88",
   "metadata": {},
   "source": [
    "## Split and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23a6729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Val / Test sizes: 5039 720 1440\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_val_data, train_labels, test_val_labels = train_test_split(\n",
    "#     paths, labels, test_size=0.3, random_state=SEED, shuffle=True, stratify=labels\n",
    "# )\n",
    "# test_data, val_data, test_labels, val_labels = train_test_split(\n",
    "#     test_val_data, test_val_labels, test_size=1/3, random_state=SEED, shuffle=True, stratify=test_val_labels\n",
    "# )\n",
    "\n",
    "indices = list(range(len(paths)))\n",
    "train_idx, test_val_idx, train_labels, test_val_labels = train_test_split(\n",
    "    indices, labels, test_size=0.3, random_state=SEED, shuffle=True, stratify=labels\n",
    ")\n",
    "test_idx, val_idx, test_labels, val_labels = train_test_split(\n",
    "    test_val_idx, test_val_labels, test_size=1/3, random_state=SEED, shuffle=True, stratify=test_val_labels\n",
    ")\n",
    "\n",
    "# print(\"Train / Val / Test sizes:\", len(train_data), len(val_data), len(test_data))\n",
    "print(\"Train / Val / Test sizes:\", len(train_idx), len(val_idx), len(test_idx))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# train_ds = FusionDataset(train_data, report_dir, label_map, tokenizer, max_length, transform)\n",
    "# val_ds   = FusionDataset(val_data,   report_dir, label_map, tokenizer, max_length, transform)\n",
    "# test_ds  = FusionDataset(test_data,  report_dir, label_map, tokenizer, max_length, transform)\n",
    "\n",
    "train_ds = CachedFusionDataset(cache_dir, train_idx)\n",
    "val_ds = CachedFusionDataset(cache_dir, val_idx)\n",
    "test_ds = CachedFusionDataset(cache_dir, test_idx)\n",
    "\n",
    "def get_loader(ds, bs, shuffle=False):\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, collate_fn=fusion_collate_fn, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e96094",
   "metadata": {},
   "source": [
    "## Concatenation Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a4898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, vision_model_name, text_model_name, vision_drop=0.1, text_drop=0.1, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.image_model = ViTModel.from_pretrained(vision_model_name)\n",
    "        self.text_model  = BertModel.from_pretrained(text_model_name)\n",
    "        self.vision_dropout = nn.Dropout(vision_drop)\n",
    "        self.text_dropout   = nn.Dropout(text_drop)\n",
    "        img_dim = self.image_model.config.hidden_size\n",
    "        txt_dim = self.text_model.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(img_dim+txt_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        img_out = self.image_model(pixel_values=pixel_values).pooler_output\n",
    "        txt_out = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        fusion = torch.cat([self.vision_dropout(img_out), self.text_dropout(txt_out)], dim=1)\n",
    "        logits = self.classifier(fusion)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb20411",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346e1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_probs):\n",
    "    metrics = {}\n",
    "    for i, name in enumerate(['edema','effusion']):\n",
    "        acc = accuracy_score(y_true[:,i], y_pred[:,i])\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true[:,i], y_pred[:,i], zero_division=0\n",
    "        )\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true[:,i], y_probs[:,i])\n",
    "        except ValueError:\n",
    "            auroc = float('nan')\n",
    "        try:\n",
    "            auprc = average_precision_score(y_true[:,i], y_probs[:,i])\n",
    "        except ValueError:\n",
    "            auprc = float('nan')\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true[:,i], y_pred[:,i]).ravel()\n",
    "        sens = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        spec = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "        metrics[name] = {\n",
    "            'accuracy': acc,\n",
    "            'precision': precision.tolist(),\n",
    "            'recall': recall.tolist(),\n",
    "            'f1': f1.tolist(),\n",
    "            'auroc': auroc,\n",
    "            'auprc': auprc,\n",
    "            'sensitivity': sens,\n",
    "            'specificity': spec\n",
    "        }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2abcd",
   "metadata": {},
   "source": [
    "## Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acf6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdf893",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321a5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            logits = model(pixel_values, input_ids, attention_mask).cpu().numpy()\n",
    "            probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probs)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_probs = np.vstack(all_probs)\n",
    "    return compute_metrics(y_true, y_pred, y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb5d9c",
   "metadata": {},
   "source": [
    "## Hyperparameter Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f970897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_combinations = []\n",
    "for vision_drop in [0.1, 0.2]:\n",
    "    for text_drop in [0.1, 0.2]:\n",
    "        for lr in [1e-5, 5e-5, 2e-4]:\n",
    "            for wd in [0, 0.01, 0.1]:\n",
    "                for bs in [32]:\n",
    "                    hyperparameter_combinations.append({\n",
    "                        'vision_drop': vision_drop,\n",
    "                        'text_drop':   text_drop,\n",
    "                        'learning_rate': lr,\n",
    "                        'weight_decay': wd,\n",
    "                        'batch_size': bs,\n",
    "                        'num_epochs': 20\n",
    "                    })\n",
    "\n",
    "results_file = 'concatenation_fusion_results.json'\n",
    "if not os.path.exists(results_file):\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump([], f)\n",
    "\n",
    "def execute_hyperparameter_combo(combo):\n",
    "    name = f\"VD{combo['vision_drop']}_TD{combo['text_drop']}_LR{combo['learning_rate']}_WD{combo['weight_decay']}_BS{combo['batch_size']}_EP{combo['num_epochs']}\"\n",
    "    print(f\"Running combo: {name}\")\n",
    "\n",
    "    train_loader = get_loader(train_ds, combo['batch_size'], shuffle=True)\n",
    "    val_loader = get_loader(val_ds, combo['batch_size'])\n",
    "    test_loader = get_loader(test_ds, combo['batch_size'])\n",
    "\n",
    "    model = FusionModel(\n",
    "        vision_model_name=model_name_vision,\n",
    "        text_model_name=model_name_text,\n",
    "        vision_drop=combo['vision_drop'],\n",
    "        text_drop=combo['text_drop']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=combo['learning_rate'],\n",
    "        weight_decay=combo['weight_decay']\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_loader) * combo['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1*total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, combo['num_epochs']+1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch}/{combo['num_epochs']} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    results.append({'name': name, 'combo': combo, 'metrics': test_metrics})\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Saved results for {name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aeef0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: VD0.2_TD0.2_LR5e-05_WD0.01_BS32_EP20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [12:52<00:00,  4.89s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:45<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.6946, Val Loss: 0.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:38<00:00,  5.18s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.6935, Val Loss: 0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:31<00:00,  5.14s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:46<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.6882, Val Loss: 0.6839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:56<00:00,  5.29s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.6792, Val Loss: 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:18<00:00,  5.05s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:46<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.6522, Val Loss: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:54<00:00,  5.28s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.5778, Val Loss: 0.5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:49<00:00,  5.25s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.4604, Val Loss: 0.3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:48<00:00,  5.24s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.3770, Val Loss: 0.3470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:43<00:00,  5.21s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.3316, Val Loss: 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:53<00:00,  5.28s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.2983, Val Loss: 0.2906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:48<00:00,  5.24s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.2717, Val Loss: 0.2728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:39<00:00,  5.56s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:50<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.2494, Val Loss: 0.2564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:45<00:00,  5.61s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:51<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.2264, Val Loss: 0.2398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:15<00:00,  5.41s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.2047, Val Loss: 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:29<00:00,  5.50s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:49<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.1736, Val Loss: 0.1923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:37<00:00,  5.55s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.1441, Val Loss: 0.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:31<00:00,  5.51s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.1228, Val Loss: 0.1606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:42<00:00,  5.59s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.1039, Val Loss: 0.1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:51<00:00,  5.64s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:50<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.0861, Val Loss: 0.1471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:21<00:00,  5.45s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.0737, Val Loss: 0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 45/45 [01:38<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics for VD0.2_TD0.2_LR5e-05_WD0.01_BS32_EP20: {'edema': {'accuracy': 0.9409722222222222, 'precision': array([0.94209891, 0.93944354]), 'recall': array([0.95476773, 0.92282958]), 'f1': array([0.94839101, 0.93106245]), 'auroc': 0.982000644659156, 'auprc': 0.976772964724611, 'sensitivity': 0.9228295819935691, 'specificity': 0.9547677261613692}, 'effusion': {'accuracy': 0.95625, 'precision': array([0.9652568 , 0.94858612]), 'recall': array([0.94108984, 0.96977661]), 'f1': array([0.95302013, 0.95906433]), 'auroc': 0.98328491888241, 'auprc': 0.9767008398933996, 'sensitivity': 0.9697766097240473, 'specificity': 0.9410898379970545}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m combo \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[43mexecute_hyperparameter_combo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombo\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36mexecute_hyperparameter_combo\u001b[0;34m(combo)\u001b[0m\n\u001b[1;32m     75\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombo\u001b[39m\u001b[38;5;124m'\u001b[39m: combo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: test_metrics})\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved results for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:429\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "combo = {\n",
    "    \"vision_drop\": 0.2,\n",
    "    \"text_drop\": 0.2,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"learning_rate\": 5e-05,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 20,\n",
    "    \"patience\": 3\n",
    "}\n",
    "execute_hyperparameter_combo(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53eaf1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: VD0.1_TD0.1_LR1e-05_WD0.1_BS32_EP20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:26<00:00,  5.10s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:51<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.6944, Val Loss: 0.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:58<00:00,  5.69s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:51<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.6949, Val Loss: 0.6939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [15:13<00:00,  5.78s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:51<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.6933, Val Loss: 0.6920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [15:09<00:00,  5.76s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:51<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.6917, Val Loss: 0.6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [15:13<00:00,  5.78s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.6879, Val Loss: 0.6866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:41<00:00,  5.20s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.6849, Val Loss: 0.6827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [16:11<00:00,  6.15s/it]\n",
      "Validation: 100%|██████████| 23/23 [01:01<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.6803, Val Loss: 0.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [19:41<00:00,  7.48s/it]\n",
      "Validation: 100%|██████████| 23/23 [01:03<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.6734, Val Loss: 0.6675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [24:10<00:00,  9.18s/it]\n",
      "Validation: 100%|██████████| 23/23 [01:06<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.6617, Val Loss: 0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [20:10<00:00,  7.66s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:50<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.6420, Val Loss: 0.6252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [15:02<00:00,  5.71s/it]\n",
      "Validation: 100%|██████████| 23/23 [01:04<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.6092, Val Loss: 0.5826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [17:19<00:00,  6.58s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:40<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.5614, Val Loss: 0.5280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [11:49<00:00,  4.49s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.5097, Val Loss: 0.4763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [12:56<00:00,  4.91s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:44<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.4623, Val Loss: 0.4331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:22<00:00,  5.08s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:47<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.4235, Val Loss: 0.4028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:08<00:00,  5.37s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:48<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.3919, Val Loss: 0.3846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:46<00:00,  5.23s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:46<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.3686, Val Loss: 0.3552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:30<00:00,  5.51s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:49<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.3500, Val Loss: 0.3462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:46<00:00,  5.61s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:46<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.3320, Val Loss: 0.3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:16<00:00,  5.04s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:46<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.3174, Val Loss: 0.3155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 45/45 [01:32<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for VD0.1_TD0.1_LR1e-05_WD0.1_BS32_EP20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combo = {\n",
    "    \"vision_drop\": 0.1,\n",
    "    \"text_drop\": 0.1,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"learning_rate\": 1e-05,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 20,\n",
    "    \"patience\": 3\n",
    "}\n",
    "execute_hyperparameter_combo(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa6e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combo: VD0.2_TD0.1_LR5e-05_WD0.01_BS32_EP20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [12:56<00:00,  4.91s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.6944, Val Loss: 0.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:36<00:00,  5.17s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:34<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 0.6931, Val Loss: 0.6904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:00<00:00,  5.32s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:34<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 0.6874, Val Loss: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:37<00:00,  5.18s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 0.6781, Val Loss: 0.6670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:46<00:00,  5.23s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 0.6465, Val Loss: 0.6122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:27<00:00,  5.11s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.5581, Val Loss: 0.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:28<00:00,  5.11s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.4431, Val Loss: 0.3919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:58<00:00,  5.31s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.3683, Val Loss: 0.3419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:59<00:00,  5.31s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.3266, Val Loss: 0.3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:17<00:00,  5.05s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:32<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.2952, Val Loss: 0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:32<00:00,  5.14s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.2695, Val Loss: 0.2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:59<00:00,  5.31s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:34<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.2473, Val Loss: 0.2559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:56<00:00,  5.29s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.2263, Val Loss: 0.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:58<00:00,  5.30s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.2070, Val Loss: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:47<00:00,  5.24s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.1792, Val Loss: 0.2038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:09<00:00,  4.99s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:32<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.1498, Val Loss: 0.1886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:01<00:00,  5.33s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:34<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.1267, Val Loss: 0.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [14:11<00:00,  5.39s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.1056, Val Loss: 0.1592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:17<00:00,  5.05s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:32<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.0888, Val Loss: 0.1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 158/158 [13:47<00:00,  5.24s/it]\n",
      "Validation: 100%|██████████| 23/23 [00:33<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.0743, Val Loss: 0.1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 45/45 [01:05<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for VD0.2_TD0.1_LR5e-05_WD0.01_BS32_EP20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combo = {\n",
    "    \"vision_drop\": 0.2,\n",
    "    \"text_drop\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"learning_rate\": 5e-05,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 20,\n",
    "    \"patience\": 3\n",
    "}\n",
    "execute_hyperparameter_combo(combo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
