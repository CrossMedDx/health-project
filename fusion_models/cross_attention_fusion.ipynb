{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300bc7d0",
   "metadata": {},
   "source": [
    "# Cross-Attention Fusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ae1f9",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92406eeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    ViTModel\n",
    ")\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------------------\n",
    "# Reproducibility\n",
    "# ---------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f0b68",
   "metadata": {},
   "source": [
    "## Load Paths & Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925eded5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_dir = \"/mnt/e/ecs289l/mimic-cxr-download/imageData/\"\n",
    "report_dir = \"../download_data/textData/\"\n",
    "labels_file = \"../download_data/metadata/edema+pleural_effusion_samples_v2.csv\"\n",
    "model_name_text = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "model_name_vision = 'google/vit-base-patch16-224-in21k'\n",
    "max_length = 128\n",
    "\n",
    "# ---------------------\n",
    "# Labels and File Loading\n",
    "# ---------------------\n",
    "meta = pd.read_csv(labels_file, dtype={'study_id': str})\n",
    "meta['study_id'] = 's' + meta['study_id']\n",
    "label_map = meta.set_index('study_id')[['edema', 'effusion']].to_dict(orient='index')\n",
    "\n",
    "all_image_paths = []\n",
    "for root, _, files in os.walk(image_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.dcm'):\n",
    "            all_image_paths.append(os.path.join(root, f))\n",
    "paths, labels = [], []\n",
    "for p in all_image_paths:\n",
    "    sid = os.path.basename(os.path.dirname(p))\n",
    "    if sid in label_map:\n",
    "        paths.append(p)\n",
    "        labels.append([label_map[sid]['edema'], label_map[sid]['effusion']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98812192",
   "metadata": {},
   "source": [
    "## Dataset Tokenizer and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565203f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name_text)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2af568",
   "metadata": {},
   "source": [
    "## Fusion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea24fa2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, image_paths, report_dir, labels_map, tokenizer, max_length, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.report_dir = report_dir\n",
    "        self.labels_map = labels_map\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        dcm = pydicom.dcmread(p)\n",
    "        arr = dcm.pixel_array.astype(np.float32)\n",
    "        img = Image.fromarray((arr/arr.max()*255).astype(np.uint8)).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        sid = os.path.basename(os.path.dirname(p))\n",
    "        report_path = os.path.join(self.report_dir, sid, 'report.txt')\n",
    "        with open(report_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        labels = self.labels_map[sid]\n",
    "        return {\n",
    "            'pixel_values': img,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Custom collate for batching\n",
    "def fusion_collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfba5d",
   "metadata": {},
   "source": [
    "## Split and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2e89b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_val_data, train_labels, test_val_labels = train_test_split(\n",
    "    paths, labels, test_size=0.3, random_state=SEED, shuffle=True, stratify=labels\n",
    ")\n",
    "test_data, val_data, test_labels, val_labels = train_test_split(\n",
    "    test_val_data, test_val_labels, test_size=1/3, random_state=SEED, shuffle=True, stratify=test_val_labels\n",
    ")\n",
    "\n",
    "print(\"Train / Val / Test sizes:\", len(train_data), len(val_data), len(test_data))\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = FusionDataset(train_paths, report_dir, label_map, tokenizer, max_length, transform)\n",
    "val_ds   = FusionDataset(val_paths,   report_dir, label_map, tokenizer, max_length, transform)\n",
    "test_ds  = FusionDataset(test_paths,  report_dir, label_map, tokenizer, max_length, transform)\n",
    "\n",
    "def get_loader(ds, bs, shuffle=False):\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, collate_fn=fusion_collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175d896",
   "metadata": {},
   "source": [
    "## Cross-Attention Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803571e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttentionFusionModel(nn.Module):\n",
    "    def __init__(self, vision_model_name, text_model_name, vision_drop=0.1, text_drop=0.1, hidden_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.image_model = ViTModel.from_pretrained(vision_model_name)\n",
    "        self.text_model  = BertModel.from_pretrained(text_model_name)\n",
    "        self.vision_proj = nn.Linear(self.image_model.config.hidden_size, hidden_dim)\n",
    "        self.text_proj   = nn.Linear(self.text_model.config.hidden_size, hidden_dim)\n",
    "        self.cross_attn  = nn.MultiheadAttention(hidden_dim, num_heads, dropout=0.1)\n",
    "        self.dropout_img = nn.Dropout(vision_drop)\n",
    "        self.dropout_txt = nn.Dropout(text_drop)\n",
    "        self.classifier  = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim//2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        img_feats = self.image_model(pixel_values=pixel_values).pooler_output  # [B, img_dim]\n",
    "        txt_feats = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output  # [B, txt_dim]\n",
    "        img_proj = self.dropout_img(self.vision_proj(img_feats)).unsqueeze(0)  # [1, B, H]\n",
    "        txt_proj = self.dropout_txt(self.text_proj(txt_feats)).unsqueeze(0)    # [1, B, H]\n",
    "        # cross-attention: image as query, text+image as key/value\n",
    "        seq = torch.cat([img_proj, txt_proj], dim=0)  # [2, B, H]\n",
    "        attn_out, _ = self.cross_attn(query=img_proj, key=seq, value=seq)  # [1, B, H]\n",
    "        fusion_vec = attn_out.squeeze(0)  # [B, H]\n",
    "        logits = self.classifier(fusion_vec)\n",
    "        return logits\n",
    "class CrossAttentionFusionModel(nn.Module):\n",
    "    def __init__(self, vision_model_name, text_model_name, vision_drop=0.1, text_drop=0.1, hidden_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.image_model = ViTModel.from_pretrained(vision_model_name)\n",
    "        self.text_model  = BertModel.from_pretrained(text_model_name)\n",
    "        self.vision_proj = nn.Linear(self.image_model.config.hidden_size, hidden_dim)\n",
    "        self.text_proj   = nn.Linear(self.text_model.config.hidden_size, hidden_dim)\n",
    "        self.cross_attn  = nn.MultiheadAttention(hidden_dim, num_heads, dropout=0.1)\n",
    "        self.dropout_img = nn.Dropout(vision_drop)\n",
    "        self.dropout_txt = nn.Dropout(text_drop)\n",
    "        self.classifier  = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim//2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        img_feats = self.image_model(pixel_values=pixel_values).pooler_output  # [B, img_dim]\n",
    "        txt_feats = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output  # [B, txt_dim]\n",
    "        img_proj = self.dropout_img(self.vision_proj(img_feats)).unsqueeze(0)  # [1, B, H]\n",
    "        txt_proj = self.dropout_txt(self.text_proj(txt_feats)).unsqueeze(0)    # [1, B, H]\n",
    "        # cross-attention: image as query, text+image as key/value\n",
    "        seq = torch.cat([img_proj, txt_proj], dim=0)  # [2, B, H]\n",
    "        attn_out, _ = self.cross_attn(query=img_proj, key=seq, value=seq)  # [1, B, H]\n",
    "        fusion_vec = attn_out.squeeze(0)  # [B, H]\n",
    "        logits = self.classifier(fusion_vec)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdb2e4",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b29961",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_probs):\n",
    "    metrics = {}\n",
    "    for i, name in enumerate(['edema','effusion']):\n",
    "        acc = accuracy_score(y_true[:,i], y_pred[:,i])\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true[:,i], y_pred[:,i], zero_division=0\n",
    "        )\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true[:,i], y_probs[:,i])\n",
    "        except ValueError:\n",
    "            auroc = float('nan')\n",
    "        try:\n",
    "            auprc = average_precision_score(y_true[:,i], y_probs[:,i])\n",
    "        except ValueError:\n",
    "            auprc = float('nan')\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true[:,i], y_pred[:,i]).ravel()\n",
    "        sens = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        spec = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "        metrics[name] = {\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auroc': auroc,\n",
    "            'auprc': auprc,\n",
    "            'sensitivity': sens,\n",
    "            'specificity': spec\n",
    "        }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a1d2e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b0bb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids     = batch['input_ids'].to(device)\n",
    "        attention_mask= batch['attention_mask'].to(device)\n",
    "        labels        = batch['labels'].to(device)\n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e939f938",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405022b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids     = batch['input_ids'].to(device)\n",
    "            attention_mask= batch['attention_mask'].to(device)\n",
    "            labels        = batch['labels'].cpu().numpy()\n",
    "            logits = model(pixel_values, input_ids, attention_mask).cpu().numpy()\n",
    "            probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probs)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = np.vstack(all_preds)\n",
    "    y_probs= np.vstack(all_probs)\n",
    "    return compute_metrics(y_true, y_pred, y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216645a",
   "metadata": {},
   "source": [
    "## Hyperparameter Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212860d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameter_combinations = []\n",
    "for vision_drop in [0.1, 0.2]:\n",
    "    for text_drop in [0.1, 0.2]:\n",
    "        for lr in [1e-5, 5e-5, 2e-4]:\n",
    "            for wd in [0, 0.01, 0.1]:\n",
    "                for bs in [16, 32]:\n",
    "                    hyperparameter_combinations.append({\n",
    "                        'vision_drop': vision_drop,\n",
    "                        'text_drop':   text_drop,\n",
    "                        'learning_rate': lr,\n",
    "                        'weight_decay': wd,\n",
    "                        'batch_size': bs,\n",
    "                        'num_epochs': 20\n",
    "                    })\n",
    "\n",
    "results_file = 'cross_attn_results.json'\n",
    "if not os.path.exists(results_file):\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump([], f)\n",
    "\n",
    "for combo in hyperparameter_combinations:\n",
    "    name = f\"CA_VD{combo['vision_drop']}_TD{combo['text_drop']}_LR{combo['learning_rate']}_WD{combo['weight_decay']}_BS{combo['batch_size']}_EP{combo['num_epochs']}\"\n",
    "    print(f\"üîß Running combo: {name}\")\n",
    "\n",
    "    train_loader = get_loader(train_ds, combo['batch_size'], shuffle=True)\n",
    "    val_loader   = get_loader(val_ds,   combo['batch_size'])\n",
    "    test_loader  = get_loader(test_ds,  combo['batch_size'])\n",
    "\n",
    "    model = CrossAttentionFusionModel(\n",
    "        vision_model_name=model_name_vision,\n",
    "        text_model_name=model_name_text,\n",
    "        vision_drop=combo['vision_drop'],\n",
    "        text_drop=combo['text_drop']\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=combo['learning_rate'],\n",
    "        weight_decay=combo['weight_decay']\n",
    "    )\n",
    "\n",
    "    total_steps = len(train_loader) * combo['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1*total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, combo['num_epochs']+1):\n",
    "        train_loss = train_epoch(model, train_loader, criterion)\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        val_loss = np.mean([m['accuracy'] for m in val_metrics.values()])\n",
    "        print(f\"Epoch {epoch}/{combo['num_epochs']} - Train Loss: {train_loss:.4f}, Val Avg Acc: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    test_metrics = evaluate(model, test_loader)\n",
    "    print(f\"üìù Test Metrics for {name}: {test_metrics}\")\n",
    "\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    results.append({'name': name, 'combo': combo, 'metrics': test_metrics})\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"‚úÖ Saved results for {name}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
